{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with for loops\n",
    "\n",
    "This is an implementation of the logistic regression. batch gradient descent is used to fit the model paramaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('max.rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impute Age NA values with mean  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robbie.morse/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:4405: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feats.Age = feats.Age.fillna(data.Age.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass   Age  SibSp  Parch     Fare\n",
       "0       3  22.0      1      0   7.2500\n",
       "1       1  38.0      1      0  71.2833\n",
       "2       3  26.0      0      0   7.9250\n",
       "3       1  35.0      1      0  53.1000\n",
       "4       3  35.0      0      0   8.0500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the features x are a n diensional vector:\n",
    "$$x \\in \\mathbb{R}^{n}$$\n",
    "    \n",
    "the weights w are a n dimensional vector\n",
    "$$w \\in \\mathbb{R}^{n}$$\n",
    "z is a linear combination of the weihts an feature values such that:\n",
    "\n",
    "$$z \\in w^{T}x$$\n",
    "    \n",
    "a s the logistic function where:\n",
    "\n",
    "$$y = a(z)$$\n",
    "\n",
    "$$a = \\frac{1}{1+exp{-z}}$$\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(feature_vector, weights):\n",
    "    \n",
    "    '''\n",
    "    takes in vector of feature values and vector of weights and computes output from logistic function\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # calculate 'z'\n",
    "    linear_combination = np.dot(feature_vector, weights)\n",
    "    \n",
    "    # input 'z' into logistic function\n",
    "    function_output = 1 / (1+np.exp(-linear_combination))\n",
    "    \n",
    "    return(function_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit logistic function with batch gradient descent\n",
    "\n",
    "loss function is $$ L(w, y) = (-[ylog(y)+(1-y)log(1-y)]$$\n",
    "cost function is $$ J = \\frac{1}{m}\\sum_{i=1}^{m} L(w, y)$$\n",
    "batch gradient descent algorithm:\n",
    "\n",
    "$$repeat\\, until\\, convergence$$\n",
    "$$\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\!\\{$$\n",
    "$$w := w - \\alpha\\frac{\\partial{J}}{\\partial{w_j}}$$\n",
    "  $$\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = \\frac{\\partial{z}}{\\partial{w_j}}*\\frac{\\partial{a}}{\\partial{z}}*\\frac{\\partial{L}}{\\partial{a}}$$\n",
    "which means one can calculate $\\frac{\\partial{L}}{\\partial{w_j}}$ int the following way\n",
    "$$\\frac{\\partial{L}}{\\partial{w_j}} = (a-y)*X_j$$\n",
    "\n",
    "In the below block of code  $\\frac{\\partial{L}}{\\partial{w_j}}$ is represented with 'dw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression(features, y, alpha = 0.05, totalIterations=100):\n",
    "\n",
    "    '''\n",
    "    fits logistic regression model to data with batch gradient descent\n",
    "    \n",
    "    features: pandas dataframe containing features\n",
    "    y: pandas series containing labels\n",
    "    alpha: learning rate\n",
    "    totalIterations: number of iterations of batch gradient descent\n",
    "    \n",
    "    '''\n",
    "\n",
    "    X = np.array(features.T)\n",
    "    X = np.insert(arr = X, values = np.ones(X.shape[1]), obj = 0, axis = 0)\n",
    "    Y = np.array(y)\n",
    "################################################################## initialise lists to store loss and cost function values   \n",
    "    loss_function_values = []\n",
    "    cost_function_values =[]\n",
    "################################################################## initialise dictionaries \n",
    "\n",
    "    \n",
    "################################################################## set up arrays\n",
    "    old_params = np.zeros((features.shape[1])+1)\n",
    "    new_params = np.zeros(features.shape[1]+1)\n",
    "    dw = np.zeros(features.shape[1]+1)\n",
    "    \n",
    "\n",
    "################################################################## set up arrays\n",
    "\n",
    "\n",
    "################################################################## loop through data \n",
    "    for counter in tqdm(range(totalIterations)):\n",
    "        \n",
    "        #reset dw to zeros\n",
    "        dw = np.zeros(features.shape[1]+1)\n",
    "        #update old paramaters with new paramaters defined from previous iteration\n",
    "        old_params = new_params.copy()\n",
    "        new_params = np.zeros(features.shape[1]+1)\n",
    "        print('X')\n",
    "        print(X.shape)\n",
    "        print(old_params)\n",
    "        print(dw)\n",
    "        print(new_params)\n",
    "        #Create vector Z which holds linear combinations of features for all observations\n",
    "        Z = np.dot(old_params.T, X)\n",
    "        #create vector A which holds outputs from logistic function for all linear combinations of features in Z\n",
    "        A = 1 / (1+np.exp(-Z))\n",
    "        #create vector A containing all errors\n",
    "        E = np.array([A - Y])\n",
    "\n",
    "################################################################## update dw\n",
    "        #record all average dw values for all features\n",
    "        dw = np.dot(E, X.T).sum(axis = 0)\n",
    "        average_dw = dw/X.shape[1]    \n",
    "################################################################## update dw\n",
    "\n",
    "\n",
    "################################################################## record loss function\n",
    "            \n",
    "        loss_function_outputs = -(Y*(np.log(A))+((1-Y)*np.log(1-A)))\n",
    "        cost_function_output = sum(loss_function_outputs)/X.shape[1]\n",
    "        cost_function_values.append(cost_function_output)\n",
    "################################################################## record loss function  \n",
    "\n",
    "\n",
    "################################################################## update feature weights\n",
    "        new_params = old_params-alpha*(average_dw)\n",
    "################################################################## update feature weights\n",
    "\n",
    "    return(new_params, cost_function_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros((feats.shape[1])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 715.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "(6, 891)\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-3.48484848e-04 -1.21717172e-03 -1.16731744e-02 -2.39057239e-04\n",
      " -3.70370370e-05  7.42173316e-03]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00060767 -0.00214204 -0.01956524 -0.00050795 -0.00010095  0.00705295]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00069111 -0.00268422 -0.02135102 -0.00069936 -0.00010618  0.01267347]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00085061 -0.00333517 -0.02541835 -0.00096842 -0.00016934  0.01086268]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00089294 -0.00374485 -0.02555038 -0.00117257 -0.000185    0.0140476 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00099279 -0.00425092 -0.02748789 -0.00142584 -0.00023607  0.01306729]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00103587 -0.0046403  -0.02753319 -0.00164664 -0.00026383  0.0144873 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00110379 -0.00507139 -0.02836052 -0.00188865 -0.00030665  0.01416781]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00114922 -0.00545539 -0.0284393  -0.0021181  -0.00034055  0.01473704]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00120289 -0.0058523  -0.02877653 -0.0023551  -0.00037977  0.01467604]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00124832 -0.00623125 -0.02883979 -0.00258769 -0.00041593  0.01489988]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00129593 -0.00661281 -0.02897172 -0.0028226  -0.00045375  0.01490853]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0013405  -0.00698729 -0.02900424 -0.00305591 -0.00049051  0.01499934]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0013854  -0.00736148 -0.0290491  -0.00328973 -0.00052769  0.01501731]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0014291  -0.00773256 -0.02905667 -0.00352284 -0.00056444  0.01505619]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00147266 -0.00810275 -0.02906315 -0.00375588 -0.00060121  0.01506976]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00151566 -0.00847132 -0.02905483 -0.00398846 -0.00063775  0.01508737]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00155846 -0.00883901 -0.02904365 -0.00422077 -0.00067417  0.01509573]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00160093 -0.00920565 -0.02902623 -0.00445269 -0.00071041  0.01510412]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0016432  -0.00957151 -0.02900666 -0.00468428 -0.0007465   0.0151089 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00168525 -0.00993657 -0.0289843  -0.00491551 -0.00078241  0.01511308]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00172712 -0.01030093 -0.02896066 -0.00514638 -0.00081817  0.01511574]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0017688  -0.01066459 -0.02893574 -0.00537688 -0.00085375  0.01511792]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00181032 -0.01102761 -0.02891012 -0.00560702 -0.00088917  0.01511942]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00185167 -0.01138999 -0.0288839  -0.0058368  -0.00092442  0.01512061]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00189287 -0.01175175 -0.02885733 -0.00606622 -0.0009595   0.01512149]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00193392 -0.01211289 -0.02883049 -0.00629527 -0.00099441  0.01512219]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00197481 -0.01247343 -0.02880349 -0.00652396 -0.00102915  0.01512274]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00201555 -0.01283338 -0.02877638 -0.00675228 -0.00106372  0.0151232 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00205614 -0.01319273 -0.02874921 -0.00698024 -0.00109813  0.01512358]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00209659 -0.01355149 -0.02872201 -0.00720784 -0.00113237  0.01512392]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00213689 -0.01390966 -0.0286948  -0.00743508 -0.00116644  0.01512422]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00217704 -0.01426724 -0.02866761 -0.00766196 -0.00120034  0.01512449]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00221705 -0.01462424 -0.02864044 -0.00788847 -0.00123408  0.01512475]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00225691 -0.01498066 -0.02861329 -0.00811462 -0.00126765  0.01512499]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00229663 -0.0153365  -0.02858619 -0.00834042 -0.00130105  0.01512522]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00233621 -0.01569176 -0.02855912 -0.00856585 -0.00133429  0.01512545]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00237564 -0.01604644 -0.02853209 -0.00879093 -0.00136736  0.01512568]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00241492 -0.01640054 -0.02850512 -0.00901564 -0.00140027  0.0151259 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00245407 -0.01675407 -0.02847818 -0.00924    -0.00143301  0.01512612]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00249307 -0.01710702 -0.0284513  -0.00946401 -0.00146559  0.01512634]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00253193 -0.0174594  -0.02842446 -0.00968765 -0.001498    0.01512656]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00257064 -0.0178112  -0.02839768 -0.00991094 -0.00153024  0.01512678]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00260921 -0.01816243 -0.02837094 -0.01013387 -0.00156233  0.015127  ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00264764 -0.01851309 -0.02834425 -0.01035645 -0.00159425  0.01512722]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00268593 -0.01886318 -0.02831762 -0.01057867 -0.001626    0.01512744]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00272408 -0.0192127  -0.02829103 -0.01080054 -0.00165759  0.01512766]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00276208 -0.01956165 -0.02826449 -0.01102206 -0.00168902  0.01512788]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00279995 -0.01991003 -0.028238   -0.01124323 -0.00172029  0.0151281 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00283767 -0.02025785 -0.02821156 -0.01146404 -0.00175139  0.01512832]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00287525 -0.0206051  -0.02818517 -0.0116845  -0.00178233  0.01512855]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0029127  -0.02095179 -0.02815883 -0.01190461 -0.00181311  0.01512877]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00295    -0.02129791 -0.02813254 -0.01212437 -0.00184373  0.015129  ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00298716 -0.02164346 -0.0281063  -0.01234378 -0.00187419  0.01512922]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00302418 -0.02198846 -0.02808011 -0.01256283 -0.00190449  0.01512945]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00306106 -0.02233289 -0.02805397 -0.01278155 -0.00193462  0.01512967]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00309781 -0.02267677 -0.02802788 -0.01299991 -0.0019646   0.0151299 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00313441 -0.02302008 -0.02800183 -0.01321792 -0.00199442  0.01513013]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00317088 -0.02336284 -0.02797584 -0.01343559 -0.00202407  0.01513036]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0032072  -0.02370503 -0.02794989 -0.01365291 -0.00205357  0.01513059]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00324339 -0.02404667 -0.02792399 -0.01386989 -0.00208291  0.01513082]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00327944 -0.02438776 -0.02789814 -0.01408652 -0.00211209  0.01513105]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00331535 -0.02472829 -0.02787234 -0.01430281 -0.00214111  0.01513129]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00335113 -0.02506826 -0.02784659 -0.01451875 -0.00216997  0.01513152]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00338676 -0.02540769 -0.02782089 -0.01473434 -0.00219868  0.01513175]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00342226 -0.02574655 -0.02779523 -0.0149496  -0.00222723  0.01513199]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00345762 -0.02608487 -0.02776962 -0.01516451 -0.00225562  0.01513222]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00349285 -0.02642264 -0.02774406 -0.01537908 -0.00228385  0.01513246]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00352793 -0.02675986 -0.02771855 -0.01559331 -0.00231193  0.01513269]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00356289 -0.02709653 -0.02769309 -0.0158072  -0.00233985  0.01513293]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0035977  -0.02743265 -0.02766768 -0.01602075 -0.00236761  0.01513317]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00363238 -0.02776822 -0.02764231 -0.01623395 -0.00239522  0.0151334 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00366692 -0.02810325 -0.02761699 -0.01644682 -0.00242267  0.01513364]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00370133 -0.02843773 -0.02759171 -0.01665935 -0.00244997  0.01513388]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00373561 -0.02877167 -0.02756649 -0.01687155 -0.00247711  0.01513412]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00376974 -0.02910507 -0.02754131 -0.0170834  -0.0025041   0.01513436]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00380375 -0.02943792 -0.02751618 -0.01729492 -0.00253093  0.0151346 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00383761 -0.02977023 -0.0274911  -0.0175061  -0.00255761  0.01513484]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00387135 -0.030102   -0.02746606 -0.01771695 -0.00258413  0.01513508]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00390495 -0.03043323 -0.02744107 -0.01792746 -0.00261051  0.01513532]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00393841 -0.03076392 -0.02741613 -0.01813763 -0.00263672  0.01513556]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00397174 -0.03109407 -0.02739123 -0.01834747 -0.00266279  0.01513581]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00400494 -0.03142369 -0.02736638 -0.01855698 -0.0026887   0.01513605]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00403801 -0.03175277 -0.02734158 -0.01876616 -0.00271446  0.01513629]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00407094 -0.03208131 -0.02731682 -0.018975   -0.00274007  0.01513654]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00410374 -0.03240932 -0.02729211 -0.01918351 -0.00276553  0.01513678]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.0041364  -0.0327368  -0.02726745 -0.01939169 -0.00279083  0.01513703]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00416894 -0.03306374 -0.02724283 -0.01959954 -0.00281599  0.01513727]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00420134 -0.03339015 -0.02721826 -0.01980706 -0.00284099  0.01513752]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00423361 -0.03371603 -0.02719374 -0.02001425 -0.00286584  0.01513776]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00426575 -0.03404138 -0.02716926 -0.02022111 -0.00289055  0.01513801]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00429775 -0.0343662  -0.02714482 -0.02042764 -0.0029151   0.01513825]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00432963 -0.03469049 -0.02712044 -0.02063384 -0.0029395   0.0151385 ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00436137 -0.03501425 -0.02709609 -0.02083972 -0.00296375  0.01513875]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00439298 -0.03533749 -0.0270718  -0.02104526 -0.00298786  0.015139  ]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00442446 -0.0356602  -0.02704755 -0.02125049 -0.00301181  0.01513924]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00445582 -0.03598238 -0.02702334 -0.02145538 -0.00303562  0.01513949]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00448704 -0.03630404 -0.02699918 -0.02165995 -0.00305927  0.01513974]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "X\n",
      "(6, 891)\n",
      "[-0.00451813 -0.03662518 -0.02697507 -0.0218642  -0.00308278  0.01513999]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights, cost_function_outputs = logisticRegression(features = feats, y = data['Survived'], alpha = 0.003, totalIterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitted paramater values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model weights are {0}'.format(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot showing output of cost function for each iteration of gradient descent\n",
    "\n",
    "There is a steady yet slow decrease in the value of the cost function. the rate of paramater fitting can be dramatically increased by vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11d2cca20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = pd.Series(cost_function_outputs) \n",
    "a.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to predict survival based on feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, weights, x_cols, sensitivity = 0.5):\n",
    "    \n",
    "    '''\n",
    "    predicts survival using fitted logistic regression model'''\n",
    "    \n",
    "    results = []\n",
    "    for row_number in range(data.shape[0]):\n",
    "\n",
    "        #loops through data and stores the feature vector and label\n",
    "        feature_vector = np.array(list(data.iloc[row_number][x_cols]))\n",
    "        feature_vector = np.insert(feature_vector, obj = [0], values = 1)\n",
    "  \n",
    "    \n",
    "        linear_combination = np.dot(feature_vector, weights)\n",
    "    \n",
    "        function_output = 1 / (1+np.exp(-linear_combination))\n",
    "        \n",
    "        if function_output >= sensitivity:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "        \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict using fitted paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict(data, weights, x_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], sensitivity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['model_prediction'] = pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('model accuracy is {0} %'.format(100*(data[data['Survived'] == data['model_prediction']].shape[0]/data.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
